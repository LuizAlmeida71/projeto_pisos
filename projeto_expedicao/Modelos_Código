Modelos_C√≥digo

Voc√™ quer criar um pipeline di√°rio que:

Recebe arquivos Excel do CISS diariamente.

Processa cada arquivo em Python, criando um DataFrame.

Adiciona uma coluna com a data de gera√ß√£o do arquivo.

Insere os dados em um Google Sheet existente, acumulando hist√≥rico.

Mant√©m o hist√≥rico apenas por 12 meses, deletando o m√™s mais antigo automaticamente.

Vou fazer duas abordagens, como voc√™ pediu:

Parte 1: Jupyter Notebook ‚Äì Simula√ß√£o de Cientista de Dados

Aqui, simulamos o processo como seria feito passo a passo em um notebook.

# Importando bibliotecas
import pandas as pd
import datetime
import gspread
from gspread_dataframe import set_with_dataframe

# --- SIMULA√á√ÉO: carregando um arquivo Excel ---
# Substitua 'relatorio.xlsx' pelo seu arquivo real
arquivo_excel = "relatorio.xlsx"
df = pd.read_excel(arquivo_excel)

# --- ADICIONANDO COLUNA DE DATA ---
data_hoje = datetime.date.today()
df['data_geracao'] = data_hoje

# --- CONECTANDO AO GOOGLE SHEET ---
# Pr√©-requisito: credenciais JSON da API do Google Sheets
gc = gspread.service_account(filename='credenciais.json')

# Abrindo planilha pelo nome
sheet = gc.open("Base_CISS").sheet1

# Lendo planilha existente para DataFrame
historico = pd.DataFrame(sheet.get_all_records())

# --- CONCATENANDO NOVOS DADOS ---
historico = pd.concat([historico, df], ignore_index=True)

# --- MANTENDO APENAS √öLTIMOS 12 MESES ---
# Considerando que 'data_geracao' √© datetime.date
historico['data_geracao'] = pd.to_datetime(historico['data_geracao'])
um_ano_atras = datetime.date.today() - pd.DateOffset(months=12)
historico = historico[historico['data_geracao'] >= um_ano_atras]

# --- ATUALIZANDO GOOGLE SHEET ---
sheet.clear()  # limpa os dados existentes
set_with_dataframe(sheet, historico)  # insere o dataframe atualizado

print("Google Sheet atualizado com sucesso!")


‚úÖ Esse c√≥digo cobre todas as etapas: leitura do Excel, adi√ß√£o de data, atualiza√ß√£o do Google Sheet e manuten√ß√£o de hist√≥rico de 12 meses.

Parte 2: Transformando em fun√ß√£o .py

Quando transformamos para .py, queremos que o c√≥digo seja modular, reutiliz√°vel e automatiz√°vel.

# arquivo: atualizar_base_ciss.py

import pandas as pd
import datetime
import gspread
from gspread_dataframe import set_with_dataframe

def atualizar_base_ciss(arquivo_excel, nome_planilha, credenciais_json):
    """
    Atualiza a base do Google Sheet com os dados do Excel.
    Mant√©m hist√≥rico apenas dos √∫ltimos 12 meses.
    
    Par√¢metros:
        arquivo_excel (str): Caminho do arquivo Excel.
        nome_planilha (str): Nome da planilha do Google Sheet.
        credenciais_json (str): Caminho do JSON de credenciais da API.
    """
    # --- Lendo Excel ---
    df = pd.read_excel(arquivo_excel)

    # --- Adicionando coluna de data ---
    data_hoje = datetime.date.today()
    df['data_geracao'] = data_hoje

    # --- Conectando ao Google Sheet ---
    gc = gspread.service_account(filename=credenciais_json)
    sheet = gc.open(nome_planilha).sheet1

    # --- Lendo planilha existente ---
    historico = pd.DataFrame(sheet.get_all_records())

    # --- Concatenando novos dados ---
    historico = pd.concat([historico, df], ignore_index=True)

    # --- Mantendo apenas √∫ltimos 12 meses ---
    historico['data_geracao'] = pd.to_datetime(historico['data_geracao'])
    um_ano_atras = pd.Timestamp.today() - pd.DateOffset(months=12)
    historico = historico[historico['data_geracao'] >= um_ano_atras]

    # --- Atualizando Google Sheet ---
    sheet.clear()
    set_with_dataframe(sheet, historico)

    print(f"Google Sheet '{nome_planilha}' atualizado com sucesso!")

# --- EXEMPLO DE USO ---
# atualizar_base_ciss("relatorio.xlsx", "Base_CISS", "credenciais.json")

‚úÖ Diferen√ßas entre notebook e .py:

Notebook: etapas expl√≠citas, visuais, ideal para explora√ß√£o.

.py: tudo encapsulado em fun√ß√£o, f√°cil de automatizar via scheduler (ex: cron job, Windows Task Scheduler).

Vari√°veis globais viram par√¢metros da fun√ß√£o.

O print final indica sucesso sem depender de c√©lulas do notebook.

Se voc√™ quiser, posso criar uma vers√£o ainda mais robusta, que:

detecta automaticamente novos arquivos na pasta di√°ria,

cria a coluna de data com base na data do arquivo se n√£o for o dia de hoje,

e mant√©m hist√≥rico sempre limpo sem precisar limpar o sheet manualmente.

Quer que eu fa√ßa essa vers√£o automatizada?

Voc√™ disse:
Esqueci uma coisa.

vamos dar uma interface em streamlit para que possa selecionar a data da gera√ß√£o do c√≥digo e mostrar que o arquivo foi carregado, pode ser? Da mesma forma como ipynb e py.
O ChatGPT disse:

Perfeito! ‚úÖ Podemos estender o c√≥digo para o Streamlit, criando uma interface simples onde o usu√°rio:

Seleciona o arquivo Excel.

Escolhe a data de gera√ß√£o (pode ser hoje ou outra data).

Carrega os dados no Google Sheet.

Mostra uma mensagem de confirma√ß√£o.

Vou fazer em tr√™s etapas, como antes: notebook, fun√ß√£o .py, e Streamlit.

Parte 1: Notebook com interface simulada (input)
import pandas as pd
import datetime
import gspread
from gspread_dataframe import set_with_dataframe

# --- SIMULANDO INPUT DO USU√ÅRIO ---
arquivo_excel = input("Informe o caminho do arquivo Excel: ")
data_input = input("Informe a data de gera√ß√£o (AAAA-MM-DD) ou deixe em branco para hoje: ")

if data_input.strip() == "":
    data_geracao = datetime.date.today()
else:
    data_geracao = pd.to_datetime(data_input).date()

# --- Lendo Excel ---
df = pd.read_excel(arquivo_excel)

# --- Adicionando coluna de data ---
df['data_geracao'] = data_geracao

# --- Conectando ao Google Sheet ---
gc = gspread.service_account(filename='credenciais.json')
sheet = gc.open("Base_CISS").sheet1

# --- Lendo planilha existente ---
historico = pd.DataFrame(sheet.get_all_records())

# --- Concatenando e mantendo √∫ltimos 12 meses ---
historico = pd.concat([historico, df], ignore_index=True)
historico['data_geracao'] = pd.to_datetime(historico['data_geracao'])
um_ano_atras = pd.Timestamp.today() - pd.DateOffset(months=12)
historico = historico[historico['data_geracao'] >= um_ano_atras]

# --- Atualizando Google Sheet ---
sheet.clear()
set_with_dataframe(sheet, historico)

print(f"Arquivo '{arquivo_excel}' carregado com sucesso para {data_geracao}!")

Parte 2: Fun√ß√£o .py
# atualizar_base_ciss.py

import pandas as pd
import datetime
import gspread
from gspread_dataframe import set_with_dataframe

def atualizar_base_ciss(arquivo_excel, nome_planilha, credenciais_json, data_geracao=None):
    """
    Atualiza a base do Google Sheet com os dados do Excel.
    
    Par√¢metros:
        arquivo_excel (str): Caminho do arquivo Excel.
        nome_planilha (str): Nome da planilha do Google Sheet.
        credenciais_json (str): Caminho do JSON de credenciais da API.
        data_geracao (str ou datetime.date, opcional): Data da gera√ß√£o do arquivo.
    """
    # --- Lendo Excel ---
    df = pd.read_excel(arquivo_excel)

    # --- Definindo data ---
    if data_geracao is None:
        data_geracao = datetime.date.today()
    else:
        data_geracao = pd.to_datetime(data_geracao).date()
    
    df['data_geracao'] = data_geracao

    # --- Conectando ao Google Sheet ---
    gc = gspread.service_account(filename=credenciais_json)
    sheet = gc.open(nome_planilha).sheet1

    # --- Lendo planilha existente ---
    historico = pd.DataFrame(sheet.get_all_records())

    # --- Concatenando e mantendo √∫ltimos 12 meses ---
    historico = pd.concat([historico, df], ignore_index=True)
    historico['data_geracao'] = pd.to_datetime(historico['data_geracao'])
    um_ano_atras = pd.Timestamp.today() - pd.DateOffset(months=12)
    historico = historico[historico['data_geracao'] >= um_ano_atras]

    # --- Atualizando Google Sheet ---
    sheet.clear()
    set_with_dataframe(sheet, historico)

    print(f"Arquivo '{arquivo_excel}' carregado com sucesso para {data_geracao}!")

Parte 3: Streamlit
# app_streamlit.py
import streamlit as st
from atualizar_base_ciss import atualizar_base_ciss
import datetime

st.title("Carregar Relat√≥rio CISS para Google Sheet")

# Upload do arquivo
arquivo = st.file_uploader("Escolha o arquivo Excel", type=['xlsx'])

# Sele√ß√£o da data de gera√ß√£o
data_geracao = st.date_input("Data de gera√ß√£o", value=datetime.date.today())

# Bot√£o de upload
if st.button("Carregar no Google Sheet"):
    if arquivo is not None:
        # Salva temporariamente o arquivo
        with open("temp.xlsx", "wb") as f:
            f.write(arquivo.getbuffer())
        
        # Atualiza a base no Google Sheet
        atualizar_base_ciss(
            arquivo_excel="temp.xlsx",
            nome_planilha="Base_CISS",
            credenciais_json="credenciais.json",
            data_geracao=data_geracao
        )
        
        st.success(f"Arquivo '{arquivo.name}' carregado com sucesso para {data_geracao}!")
    else:
        st.warning("Por favor, escolha um arquivo Excel primeiro.")

‚úÖ O que conseguimos com isso:

Notebook: simula inputs e mostra passo a passo.

Fun√ß√£o .py: modular, reutiliz√°vel, pode ser chamada por scheduler.

Streamlit: interface amig√°vel, permite selecionar data, subir arquivo e receber confirma√ß√£o visual.

Se voc√™ quiser, posso melhorar ainda mais a vers√£o do Streamlit para:

mostrar preview dos dados carregados,

listar os √∫ltimos 5 arquivos carregados,

e avisar se algum dado do m√™s mais antigo foi deletado.

Quer que eu fa√ßa essa vers√£o avan√ßada?

Voc√™ disse:
Pergunta. A base vai servir para alimentar o looker, as agrega√ß√µes necess√°rias devem ser feitas nessa ABT forjada no python antes de subir para a nuvem?
O ChatGPT disse:

√ìtima pergunta! üîç Vamos analisar cuidadosamente.

Voc√™ quer alimentar o Looker (ou outra ferramenta de BI) com a base que estamos criando no Python. A d√∫vida √©: devemos fazer as agrega√ß√µes no Python ou deixar para o Looker?

1Ô∏è‚É£ Quando fazer agrega√ß√µes no Python (antes de subir para a nuvem)

Voc√™ faz agrega√ß√µes no Python quando:

Quer reduzir o volume de dados enviado para a nuvem, economizando espa√ßo e tempo de query.

As agrega√ß√µes s√£o fixas e padronizadas, sempre usadas da mesma forma (ex: total di√°rio de vendas).

Quer ter uma ABT (Analytical Base Table) pronta, com todos os c√°lculos j√° feitos, que simplifica o trabalho do Looker.

Exemplo de agrega√ß√µes no Python:

# Suponha que df j√° tem a coluna 'valor' e 'categoria'
df_agg = df.groupby(['data_geracao', 'categoria']).agg({'valor':'sum'}).reset_index()


Isso cria uma tabela resumida, pronta para an√°lises no Looker.

2Ô∏è‚É£ Quando deixar para o Looker

Voc√™ deixa os dados ‚Äúcrus‚Äù (sem agrega√ß√£o) quando:

Quer flexibilidade total nas an√°lises, diferentes usu√°rios podem precisar de m√©tricas diferentes.

O volume de dados n√£o √© t√£o grande e o Looker pode lidar com a base inteira sem lentid√£o.

Quer poder explorar todas as dimens√µes e granularidades depois, sem precisar recriar a ABT.

üí° Resumindo

Dados j√° agregados no Python ‚Üí mais r√°pido, leve, pronto para dashboards padronizados.

Dados crus ‚Üí mais flex√≠vel, Looker faz as agrega√ß√µes dinamicamente.

Sugest√£o pr√°tica:

Mantenha duas tabelas na nuvem:

ciss_raw: todos os dados crus do Excel (hist√≥rico completo de 12 meses).

ciss_abt: tabela agregada feita no Python, pronta para dashboards mais r√°pidos.

Assim voc√™ tem flexibilidade e performance ao mesmo tempo.

Se voc√™ quiser, posso te mostrar como modificar o pipeline Python/Streamlit para gerar automaticamente a ABT agregada, mantendo os dados crus tamb√©m. Isso deixaria a base pronta para o Looker sem perder flexibilidade.

Quer que eu fa√ßa isso?

Voc√™ disse:
Mostre
O ChatGPT disse:

Perfeito! Vamos criar um pipeline que gera duas tabelas:

ciss_raw ‚Äì todos os dados crus, mantendo hist√≥rico de 12 meses.

ciss_abt ‚Äì ABT agregada, pronta para dashboards Looker.

Vou mostrar a vers√£o Python modular, que pode ser usada no .py ou no Streamlit.

1Ô∏è‚É£ Fun√ß√£o para atualizar base RAW
import pandas as pd
import datetime
import gspread
from gspread_dataframe import set_with_dataframe

def atualizar_base_raw(arquivo_excel, nome_planilha, credenciais_json, data_geracao=None):
    """
    Atualiza a base RAW do Google Sheet com os dados do Excel.
    Mant√©m hist√≥rico de 12 meses.
    """
    # Lendo Excel
    df = pd.read_excel(arquivo_excel)

    # Definindo data
    if data_geracao is None:
        data_geracao = datetime.date.today()
    else:
        data_geracao = pd.to_datetime(data_geracao).date()
    df['data_geracao'] = data_geracao

    # Conectando ao Google Sheet
    gc = gspread.service_account(filename=credenciais_json)
    sheet = gc.open(nome_planilha).sheet1

    # Lendo dados existentes
    historico = pd.DataFrame(sheet.get_all_records())

    # Concatenando e mantendo √∫ltimos 12 meses
    historico = pd.concat([historico, df], ignore_index=True)
    historico['data_geracao'] = pd.to_datetime(historico['data_geracao'])
    um_ano_atras = pd.Timestamp.today() - pd.DateOffset(months=12)
    historico = historico[historico['data_geracao'] >= um_ano_atras]

    # Atualizando planilha
    sheet.clear()
    set_with_dataframe(sheet, historico)

    return historico  # Retorna o dataframe atualizado

2Ô∏è‚É£ Fun√ß√£o para gerar ABT agregada

Supondo que queremos agregar por data e categoria, somando valores:

def gerar_abt(df_raw, col_grupo='categoria', col_valor='valor', nome_planilha_abt='CISS_ABT', credenciais_json='credenciais.json'):
    """
    Cria a ABT agregada a partir do dataframe RAW e envia para o Google Sheet.
    """
    # Agrupando dados
    df_abt = df_raw.groupby(['data_geracao', col_grupo]).agg({col_valor:'sum'}).reset_index()

    # Conectando ao Google Sheet
    gc = gspread.service_account(filename=credenciais_json)
    sheet_abt = gc.open(nome_planilha_abt).sheet1

    # Atualizando planilha ABT
    sheet_abt.clear()
    set_with_dataframe(sheet_abt, df_abt)

    return df_abt

3Ô∏è‚É£ Fun√ß√£o principal para pipeline completo
def pipeline_ciss(arquivo_excel, nome_planilha_raw, nome_planilha_abt, credenciais_json, data_geracao=None):
    """
    Pipeline completo: atualiza RAW e gera ABT agregada.
    """
    # Atualiza base RAW
    df_raw = atualizar_base_raw(
        arquivo_excel=arquivo_excel,
        nome_planilha=nome_planilha_raw,
        credenciais_json=credenciais_json,
        data_geracao=data_geracao
    )

    # Gera ABT agregada
    df_abt = gerar_abt(
        df_raw=df_raw,
        col_grupo='categoria',  # ajuste conforme seu dataframe
        col_valor='valor',
        nome_planilha_abt=nome_planilha_abt,
        credenciais_json=credenciais_json
    )

    print(f"Pipeline conclu√≠do! {arquivo_excel} carregado para RAW e ABT.")
    return df_raw, df_abt

‚úÖ Como usar no Streamlit
import streamlit as st
from pipeline_ciss import pipeline_ciss
import datetime

st.title("Carregar Relat√≥rio CISS com ABT para Looker")

arquivo = st.file_uploader("Escolha o arquivo Excel", type=['xlsx'])
data_geracao = st.date_input("Data de gera√ß√£o", value=datetime.date.today())

if st.button("Carregar e gerar ABT"):
    if arquivo is not None:
        with open("temp.xlsx", "wb") as f:
            f.write(arquivo.getbuffer())

        df_raw, df_abt = pipeline_ciss(
            arquivo_excel="temp.xlsx",
            nome_planilha_raw="CISS_RAW",
            nome_planilha_abt="CISS_ABT",
            credenciais_json="credenciais.json",
            data_geracao=data_geracao
        )

        st.success(f"Arquivo '{arquivo.name}' carregado com sucesso!")
        st.write("√öltimos registros da ABT:")
        st.dataframe(df_abt.tail(10))
    else:
        st.warning("Por favor, escolha um arquivo Excel primeiro.")

‚úÖ O que conseguimos com isso

Mantemos dados crus (CISS_RAW) para flexibilidade.

Criamos ABT agregada (CISS_ABT) pronta para dashboards Looker.

Hist√≥rico sempre limitado a 12 meses.

Streamlit fornece interface amig√°vel para upload e visualiza√ß√£o.

Se voc√™ quiser, posso fazer uma vers√£o da ABT mais avan√ßada, que:

agrega v√°rias m√©tricas (soma, m√©dia, contagem),

calcula KPIs di√°rios,

j√° prepara campos para Looker,

mantendo flexibilidade para explorar diferentes dimens√µes.

Quer que eu fa√ßa essa vers√£o avan√ßada da ABT?

python tratamento_dados_produto.py